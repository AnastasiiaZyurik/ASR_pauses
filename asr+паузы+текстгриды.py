# -*- coding: utf-8 -*-
"""ASR+–ø–∞—É–∑—ã+—Ç–µ–∫—Å—Ç–≥—Ä–∏–¥—ã.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-ALsotoRrecKVtGxf7KMge2WvEu9K2b0

**–ü–æ–ª–Ω—ã–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∫–æ–¥**
"""

!pip install librosa numpy torch torchaudio tqdm textgrid openai-whisper

import librosa
import numpy as np
import whisper
import warnings
from tqdm import tqdm
from pathlib import Path
from textgrid import TextGrid, IntervalTier

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
DATA_DIR = "/content/audio"
SAMPLE_RATE = 16000
MIN_SILENCE_DURATION = 0.3
PAUSE_TOKEN = lambda d: f"( {round(d,2)} )"

warnings.filterwarnings("ignore", message="FP16 is not supported on CPU")
whisper_model = whisper.load_model("medium")

output_dir = Path(DATA_DIR) / "textgrids"
output_dir.mkdir(exist_ok=True)

def analyze_pauses(y, sr, words):
    frame_length = int(sr * 0.02)
    hop_length = int(sr * 0.01)
    energy = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
    times = librosa.frames_to_time(np.arange(len(energy)), sr=sr, hop_length=hop_length)
    silence_threshold = np.median(energy) * 0.5

    pauses = []
    in_silence = False
    silence_start = 0

    for t, e in zip(times, energy):
        if e < silence_threshold and not in_silence:
            in_silence = True
            silence_start = t
        elif e >= silence_threshold and in_silence:
            in_silence = False
            silence_duration = t - silence_start
            if silence_duration >= MIN_SILENCE_DURATION:
                pauses.append({
                    "start": silence_start,
                    "end": t,
                    "duration": silence_duration,
                    "source": "librosa"
                })
    return pauses

def adjust_segments(segments):
    """–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã, —É—Å—Ç—Ä–∞–Ω—è—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è"""
    if not segments:
        return []

    # –°–Ω–∞—á–∞–ª–∞ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—á–∞–ª–∞
    segments.sort(key=lambda x: x["start"])

    adjusted = []
    current = segments[0].copy()

    for next_seg in segments[1:]:
        if current["end"] > next_seg["start"]:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ, —Ä–∞–∑–¥–µ–ª—è–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã
            adjusted.append({
                "text": current["text"],
                "start": current["start"],
                "end": next_seg["start"]
            })
            current = next_seg.copy()
        else:
            adjusted.append(current)
            current = next_seg.copy()
    adjusted.append(current)

    # –£–¥–∞–ª—è–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã —Å –Ω—É–ª–µ–≤–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é
    return [s for s in adjusted if s["end"] - s["start"] > 0.001]

def process_audio(file_path):
    file_path = Path(file_path)

    try:
        y, sr = librosa.load(file_path, sr=SAMPLE_RATE)
        duration = len(y) / sr

        result = whisper_model.transcribe(str(file_path), word_timestamps=True)
        segments = result.get("segments", [])

        words = []
        for seg in segments:
            if "words" in seg and isinstance(seg["words"], list):
                for word in seg["words"]:
                    if isinstance(word, dict) and "word" in word:
                        words.append({
                            "text": word.get("word", "").strip(),
                            "start": word.get("start", 0),
                            "end": word.get("end", 0)
                        })
            else:
                words.append({
                    "text": seg.get("text", "").strip(),
                    "start": seg.get("start", 0),
                    "end": seg.get("end", 0)
                })

        if not words:
            words.append({
                "text": "[–¢–µ–∫—Å—Ç –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω]",
                "start": 0,
                "end": duration
            })

        # –ê–Ω–∞–ª–∏–∑ –ø–∞—É–∑
        whisper_pauses = []
        for i in range(len(words)-1):
            pause_duration = words[i+1]["start"] - words[i]["end"]
            if pause_duration >= MIN_SILENCE_DURATION:
                whisper_pauses.append({
                    "start": words[i]["end"],
                    "end": words[i+1]["start"],
                    "duration": pause_duration,
                    "source": "whisper"
                })

        librosa_pauses = analyze_pauses(y, sr, words)

        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–∞—É–∑
        all_pauses = whisper_pauses + librosa_pauses
        all_pauses.sort(key=lambda x: x["start"])

        unique_pauses = []
        for pause in all_pauses:
            if not unique_pauses or pause["start"] >= unique_pauses[-1]["end"]:
                unique_pauses.append(pause)
            else:
                unique_pauses[-1]["end"] = max(unique_pauses[-1]["end"], pause["end"])
                unique_pauses[-1]["duration"] = unique_pauses[-1]["end"] - unique_pauses[-1]["start"]
                unique_pauses[-1]["source"] = "combined"

        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        events = []
        for word in words:
            events.append({
                "type": "word",
                "text": word["text"],
                "start": word["start"],
                "end": word["end"]
            })

        for pause in unique_pauses:
            events.append({
                "type": "pause",
                "text": PAUSE_TOKEN(pause["duration"]),
                "start": pause["start"],
                "end": pause["end"]
            })

        events.sort(key=lambda x: x["start"])

        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–π
        segments = []
        current_time = 0.0

        for event in events:
            if current_time < event["start"]:
                segments.append({
                    "text": "",
                    "start": current_time,
                    "end": event["start"]
                })

            segments.append({
                "text": event["text"],
                "start": event["start"],
                "end": event["end"]
            })
            current_time = event["end"]

        if current_time < duration:
            segments.append({
                "text": "",
                "start": current_time,
                "end": duration
            })

        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        segments = adjust_segments(segments)

        # –°–æ–∑–¥–∞–Ω–∏–µ TextGrid
        tg = TextGrid(minTime=0, maxTime=duration)
        tier = IntervalTier(name="transcription", minTime=0, maxTime=duration)

        for s in segments:
            if s["end"] > s["start"]:
                tier.add(s["start"], s["end"], s["text"])

        tg.append(tier)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        out_path = output_dir / (file_path.stem + ".TextGrid")
        tg.write(str(out_path))
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {file_path.name}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path.name}: {str(e)}")

if __name__ == "__main__":
    wav_files = sorted(f for f in Path(DATA_DIR).glob("*") if f.suffix.lower() == ".wav")
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(wav_files)} WAV-—Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")

    for wav in tqdm(wav_files):
        target_path = output_dir / (wav.stem + ".TextGrid")
        if not target_path.exists():
            process_audio(wav)
        else:
            print(f"‚è© –ü—Ä–æ–ø—É—Å–∫ (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω): {wav.name}")

    print("\nüéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:", output_dir)

!pip install librosa
import librosa
from pathlib import Path


AUDIO_DIR = Path("/content/audio")


wav_files = sorted(AUDIO_DIR.glob("*.wav"))

total_duration = 0.0

print("–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤:\n")

for wav in wav_files:
    try:
        y, sr = librosa.load(wav, sr=None)
        duration = len(y) / sr
        total_duration += duration
        print(f"{wav.name}: {duration:.2f} —Å–µ–∫")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {wav.name}: {e}")

print(f"\nüìä –û–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {total_duration:.2f} —Å–µ–∫ ({total_duration/60:.2f} –º–∏–Ω)")